{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim                             \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to C:\\Users\\user/Downloads/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [18:56<00:00, 150013.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\user/Downloads/cifar-10-python.tar.gz to C:\\Users\\user/Downloads/\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# SD of all pixels should be 1\n",
    "# 0.5, 0.5, 0.5 -- RGB\n",
    "# first set of 0.5 0.5 0.5 represents the means of each channel\n",
    "# second set of 0.5 0.5 0.5 represents the SDs of each channel\n",
    "# for each pixel value, subtract from it 0.5 (mean; need to get the mean from the training examples) and divide it by 0.5 (dividing so that when we compute the SD, it's going to give us 1)\n",
    "# the transform function automatically does it, vals between -1 and 1, SD = 1\n",
    "\n",
    "trainset = datasets.CIFAR10(\n",
    "    root = '~/Downloads/',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform \n",
    ")\n",
    "testset = datasets.MNIST(\n",
    "    root = '~/Downloads/',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transform \n",
    ")\n",
    "\n",
    "batchSize = 64\n",
    "train_dataloader = DataLoader(trainset, batch_size = batchSize, shuffle = True)\n",
    "test_dataloader = DataLoader(testset, batch_size = 8, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]             # returns X (image) and Y (label)\n",
    "trainset[0][0]          # image is contained at index 0\n",
    "trainset.classes        # returns a list of all classes\n",
    "trainset[0][0].shape    # size of the image is 3 x 32 x 32 (in PyTorch, depth is first), but you need to reshape if you want to view it using plt\n",
    "                        # instead of all this, we'll use a transformation, normalize it but with -1 instead of the mean and 2 for SD\n",
    "                        # this will give us back the original image (see next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transform_back \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToPILImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# PIL takes care of changing the shape and the visualization\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(transform_back(trainset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "transform_back = transforms.Compose([transforms.ToTensor(), transforms.Normalize((-1, -1, -1), (2, 2, 2))], transforms.ToPILImage())\n",
    "# PIL takes care of changing the shape and the visualization\n",
    "\n",
    "plt.imshow(transform_back(trainset[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
